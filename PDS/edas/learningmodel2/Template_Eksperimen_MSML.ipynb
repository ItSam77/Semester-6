{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "#Type your code here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "#Type your code here\n",
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.head()\n",
        "\n",
        "print(\"\\nLast 5 rows:\")\n",
        "df.tail()\n",
        "\n",
        "print(\"\\nDataset Description:\")\n",
        "df.describe()\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nUnique values in each column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "\n",
        "# Check for duplicates\n",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Visualizations\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Distribution of numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "if len(numerical_cols) > 0:\n",
        "    plt.subplot(2, 2, 1)\n",
        "    df[numerical_cols].hist(bins=20, figsize=(15, 10))\n",
        "    plt.suptitle('Distribution of Numerical Features')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Correlation heatmap for numerical features\n",
        "if len(numerical_cols) > 1:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    correlation_matrix = df[numerical_cols].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation Heatmap of Numerical Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Distribution of categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    fig, axes = plt.subplots(nrows=(len(categorical_cols)+2)//3, ncols=3, figsize=(15, 5*((len(categorical_cols)+2)//3)))\n",
        "    axes = axes.flatten() if len(categorical_cols) > 3 else [axes] if len(categorical_cols) == 1 else axes\n",
        "    \n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        if i < len(axes):\n",
        "            df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
        "            axes[i].set_title(f'Distribution of {col}')\n",
        "            axes[i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Check for outliers using boxplots\n",
        "if len(numerical_cols) > 0:\n",
        "    fig, axes = plt.subplots(nrows=(len(numerical_cols)+2)//3, ncols=3, figsize=(15, 5*((len(numerical_cols)+2)//3)))\n",
        "    axes = axes.flatten() if len(numerical_cols) > 3 else [axes] if len(numerical_cols) == 1 else axes\n",
        "    \n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        if i < len(axes):\n",
        "            sns.boxplot(y=df[col], ax=axes[i])\n",
        "            axes[i].set_title(f'Boxplot of {col}')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Analisis Hubungan Antar Variabel Kategorikal (Selain Target)\n",
        "\n",
        "# Get categorical columns excluding the target variable\n",
        "categorical_cols_analysis = [col for col in categorical_cols if col != 'Attrition']\n",
        "\n",
        "if len(categorical_cols_analysis) > 1:\n",
        "    print(\"=== ANALISIS HUBUNGAN ANTAR VARIABEL KATEGORIKAL ===\\n\")\n",
        "    \n",
        "    # Chi-square test for independence between categorical variables\n",
        "    from scipy.stats import chi2_contingency\n",
        "    \n",
        "    # Create a correlation-like matrix for categorical variables using Cramér's V\n",
        "    def cramers_v(x, y):\n",
        "        \"\"\"Calculate Cramér's V statistic for categorical-categorical association\"\"\"\n",
        "        confusion_matrix = pd.crosstab(x, y)\n",
        "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "        n = confusion_matrix.sum().sum()\n",
        "        phi2 = chi2 / n\n",
        "        r, k = confusion_matrix.shape\n",
        "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "        rcorr = r - ((r-1)**2)/(n-1)\n",
        "        kcorr = k - ((k-1)**2)/(n-1)\n",
        "        return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "    \n",
        "    # Calculate Cramér's V matrix\n",
        "    cramers_matrix = pd.DataFrame(index=categorical_cols_analysis, columns=categorical_cols_analysis)\n",
        "    \n",
        "    for col1 in categorical_cols_analysis:\n",
        "        for col2 in categorical_cols_analysis:\n",
        "            if col1 == col2:\n",
        "                cramers_matrix.loc[col1, col2] = 1.0\n",
        "            else:\n",
        "                cramers_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
        "    \n",
        "    # Convert to numeric\n",
        "    cramers_matrix = cramers_matrix.astype(float)\n",
        "    \n",
        "    # Plot Cramér's V heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cramers_matrix, annot=True, cmap='viridis', center=0.5, \n",
        "                fmt='.3f', square=True, cbar_kws={'label': \"Cramér's V\"})\n",
        "    plt.title(\"Cramér's V Heatmap - Categorical Variables Association\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Detailed analysis of strong associations (Cramér's V > 0.3)\n",
        "    print(\"Pasangan variabel dengan asosiasi kuat (Cramér's V > 0.3):\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    strong_associations = []\n",
        "    for i, col1 in enumerate(categorical_cols_analysis):\n",
        "        for j, col2 in enumerate(categorical_cols_analysis):\n",
        "            if i < j:  # Avoid duplicates\n",
        "                cramers_val = cramers_matrix.loc[col1, col2]\n",
        "                if cramers_val > 0.3:\n",
        "                    strong_associations.append((col1, col2, cramers_val))\n",
        "                    print(f\"{col1} vs {col2}: Cramér's V = {cramers_val:.3f}\")\n",
        "    \n",
        "    if not strong_associations:\n",
        "        print(\"Tidak ada pasangan variabel dengan asosiasi kuat (Cramér's V > 0.3)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Visualize relationships for the strongest associations\n",
        "    if strong_associations:\n",
        "        # Sort by Cramér's V and take top 6\n",
        "        strong_associations.sort(key=lambda x: x[2], reverse=True)\n",
        "        top_associations = strong_associations[:6]\n",
        "        \n",
        "        print(f\"\\nVisualisasi {len(top_associations)} asosiasi terkuat:\")\n",
        "        \n",
        "        fig, axes = plt.subplots(nrows=(len(top_associations)+2)//3, ncols=3, \n",
        "                                figsize=(15, 5*((len(top_associations)+2)//3)))\n",
        "        if len(top_associations) == 1:\n",
        "            axes = [axes]\n",
        "        elif len(top_associations) <= 3:\n",
        "            axes = axes if isinstance(axes, (list, np.ndarray)) else [axes]\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "        \n",
        "        for i, (col1, col2, cramers_val) in enumerate(top_associations):\n",
        "            if i < len(axes):\n",
        "                # Create contingency table\n",
        "                ct = pd.crosstab(df[col1], df[col2])\n",
        "                \n",
        "                # Create heatmap of the contingency table\n",
        "                sns.heatmap(ct, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "                axes[i].set_title(f'{col1} vs {col2}\\n(Cramér\\'s V = {cramers_val:.3f})')\n",
        "                axes[i].set_xlabel(col2)\n",
        "                axes[i].set_ylabel(col1)\n",
        "        \n",
        "        # Hide unused subplots\n",
        "        for j in range(len(top_associations), len(axes)):\n",
        "            axes[j].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Tidak cukup variabel kategorikal untuk analisis hubungan (minimum 2 variabel dibutuhkan)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of target variable (Attrition) across categorical features\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "categorical_cols = [col for col in categorical_cols if col != 'Attrition']  # Exclude target variable\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    fig, axes = plt.subplots(nrows=(len(categorical_cols)+2)//3, ncols=3, figsize=(15, 5*((len(categorical_cols)+2)//3)))\n",
        "    axes = axes.flatten() if len(categorical_cols) > 3 else [axes] if len(categorical_cols) == 1 else axes\n",
        "    \n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        if i < len(axes):\n",
        "            # Create crosstab for better visualization\n",
        "            ct = pd.crosstab(df[col], df['Attrition'], normalize='index') * 100\n",
        "            ct.plot(kind='bar', ax=axes[i], stacked=True)\n",
        "            axes[i].set_title(f'Attrition Distribution by {col}')\n",
        "            axes[i].set_ylabel('Percentage')\n",
        "            axes[i].tick_params(axis='x', rotation=45)\n",
        "            axes[i].legend(title='Attrition')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Distribution of target variable (Attrition) across numerical features\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "if len(numerical_cols) > 0:\n",
        "    fig, axes = plt.subplots(nrows=(len(numerical_cols)+2)//3, ncols=3, figsize=(15, 5*((len(numerical_cols)+2)//3)))\n",
        "    axes = axes.flatten() if len(numerical_cols) > 3 else [axes] if len(numerical_cols) == 1 else axes\n",
        "    \n",
        "    # Define consistent colors for attrition groups\n",
        "    colors = ['skyblue', 'salmon']\n",
        "    \n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        if i < len(axes):\n",
        "            # Create histogram for each attrition group\n",
        "            for j, attrition_val in enumerate(df['Attrition'].unique()):\n",
        "                subset = df[df['Attrition'] == attrition_val]\n",
        "                axes[i].hist(subset[col], alpha=0.7, label=f'Attrition: {attrition_val}', bins=20, color=colors[j])\n",
        "            \n",
        "            axes[i].set_title(f'Distribution of {col} by Attrition')\n",
        "            axes[i].set_xlabel(col)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].legend()\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlations between features\n",
        "\n",
        "# Get numerical and categorical columns after preprocessing\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove target variable from analysis\n",
        "if 'Attrition' in numerical_cols:\n",
        "    numerical_cols.remove('Attrition')\n",
        "\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "# 1. Numerical-Numerical correlations\n",
        "print(\"\\n=== Numerical-Numerical Correlations ===\")\n",
        "numerical_corr = df[numerical_cols].corr()\n",
        "\n",
        "# Find highly correlated numerical pairs (correlation > 0.7 or < -0.7)\n",
        "high_corr_pairs = []\n",
        "for i in range(len(numerical_corr.columns)):\n",
        "    for j in range(i+1, len(numerical_corr.columns)):\n",
        "        corr_value = numerical_corr.iloc[i, j]\n",
        "        if abs(corr_value) > 0.7:\n",
        "            high_corr_pairs.append((numerical_corr.columns[i], numerical_corr.columns[j], corr_value))\n",
        "\n",
        "print(\"Highly correlated numerical feature pairs (|correlation| > 0.7):\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"{pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "# 2. Categorical-Categorical correlations using Cramér's V\n",
        "print(\"\\n=== Categorical-Categorical Correlations ===\")\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculate Cramér's V statistic for categorical-categorical association\"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "if len(categorical_cols) > 1:\n",
        "    from scipy.stats import chi2_contingency\n",
        "    \n",
        "    categorical_corr_pairs = []\n",
        "    for i in range(len(categorical_cols)):\n",
        "        for j in range(i+1, len(categorical_cols)):\n",
        "            cramers_val = cramers_v(df[categorical_cols[i]], df[categorical_cols[j]])\n",
        "            if cramers_val > 0.3:  # Threshold for moderate association\n",
        "                categorical_corr_pairs.append((categorical_cols[i], categorical_cols[j], cramers_val))\n",
        "    \n",
        "    print(\"Highly associated categorical feature pairs (Cramér's V > 0.3):\")\n",
        "    for pair in categorical_corr_pairs:\n",
        "        print(f\"{pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")\n",
        "else:\n",
        "    print(\"Not enough categorical columns for correlation analysis\")\n",
        "\n",
        "\n",
        "# Summary of all correlated features\n",
        "print(\"\\n=== SUMMARY: All Correlated Feature Pairs ===\")\n",
        "all_correlated_features = set()\n",
        "\n",
        "for pair in high_corr_pairs:\n",
        "    all_correlated_features.add(pair[0])\n",
        "    all_correlated_features.add(pair[1])\n",
        "    print(f\"Numerical-Numerical: {pair[0]} <-> {pair[1]} (r={pair[2]:.3f})\")\n",
        "\n",
        "if len(categorical_cols) > 1:\n",
        "    for pair in categorical_corr_pairs:\n",
        "        all_correlated_features.add(pair[0])\n",
        "        all_correlated_features.add(pair[1])\n",
        "        print(f\"Categorical-Categorical: {pair[0]} <-> {pair[1]} (V={pair[2]:.3f})\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nFeatures involved in correlations: {sorted(list(all_correlated_features))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping on attrition column Yes : 1 and No : 0\n",
        "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
        "# Change the data type of Attrition column to int\n",
        "df['Attrition'] = df['Attrition'].astype(int)\n",
        "# Mapping also on OverTime column Yes : 1 and No : 0\n",
        "df['OverTime'] = df['OverTime'].map({'Yes': 1, 'No': 0})\n",
        "# Change the data type of OverTime column to int\n",
        "df['OverTime'] = df['OverTime'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "# Remove the data if it contains any missing values and duplicates\n",
        "df = df.dropna().drop_duplicates()\n",
        "# Remove the data if it contains only 1 unique value\n",
        "df = df.loc[:, df.nunique() > 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop redundant features based on analysis\n",
        "redundant_features = [\n",
        "    'EmployeeNumber',      # ID column, not useful for prediction\n",
        "    'MonthlyIncome',       # Redundant with JobLevel\n",
        "    'PercentSalaryHike',   # Redundant with PerformanceRating\n",
        "    'YearsInCurrentRole',  # Redundant with YearsAtCompany\n",
        "    'YearsWithCurrManager',# Redundant with YearsAtCompany\n",
        "    'Department'           # Redundant with JobRole\n",
        "]\n",
        "\n",
        "# Remove redundant features from the dataset\n",
        "df = df.drop(columns=redundant_features, errors='ignore')\n",
        "\n",
        "print(f\"Dropped {len(redundant_features)} redundant features\")\n",
        "print(f\"Remaining features: {df.shape[1]}\")\n",
        "print(f\"Features dropped: {redundant_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a new column LoyaltyRatio with YearsatCompany divided by Total Working Years\n",
        "df['LoyaltyRatio'] = df['YearsAtCompany'] / df['TotalWorkingYears']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows where LoyaltyRatio is null because isn't applicable\n",
        "df = df.dropna(subset=['LoyaltyRatio'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Get numerical columns (excluding binary encoded columns)\n",
        "# numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "# numerical_cols = [col for col in numerical_cols if not col.startswith(('Department_', 'Gender_')) and col != 'Attrition']\n",
        "\n",
        "# # Initialize StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# # Apply standardization to numerical columns\n",
        "# df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encoding for categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Get categorical columns (excluding already encoded columns)\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "categorical_cols = [col for col in categorical_cols if not col.startswith(('Department_', 'Gender_'))]\n",
        "\n",
        "# List mapping encoded columns to their original values\n",
        "# Store mapping for each column during encoding\n",
        "encoded_columns_mapping = {}\n",
        "for col in categorical_cols:\n",
        "    # Fit the encoder and transform the data\n",
        "    temp_encoder = LabelEncoder()\n",
        "    df[col] = temp_encoder.fit_transform(df[col])\n",
        "    encoded_columns_mapping[col] = dict(zip(temp_encoder.classes_, range(len(temp_encoder.classes_))))\n",
        "\n",
        "print(\"\\nEncoded columns mapping:\")\n",
        "for col, mapping in encoded_columns_mapping.items():\n",
        "    print(f\"{col}: {mapping}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Selection using SelectKBest\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Separate features and target variable for feature selection\n",
        "X_temp = df.drop(columns=['Attrition'])\n",
        "y_temp = df['Attrition']\n",
        "\n",
        "# Apply SelectKBest with f_classif for classification\n",
        "# Select top 20 features (you can adjust this number based on your needs)\n",
        "k_best = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = k_best.fit_transform(X_temp, y_temp)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_features = X_temp.columns[k_best.get_support()]\n",
        "print(f\"Selected {len(selected_features)} features:\")\n",
        "print(selected_features.tolist())\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X_temp.columns,\n",
        "    'Score': k_best.scores_,\n",
        "    'Selected': k_best.get_support()\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 features by score:\")\n",
        "print(feature_scores.head(10))\n",
        "\n",
        "# Update the dataframe to include only selected features plus target\n",
        "df = df[list(selected_features) + ['Attrition']]\n",
        "print(f\"\\nDataFrame shape after feature selection: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the processed data to CSV\n",
        "df.to_csv('processed_data.csv', index=False)\n",
        "print(\"✅ Processed data saved to 'processed_data.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
